{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest and SVM baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTq-DmMUhPkg",
        "colab_type": "text"
      },
      "source": [
        "## How to run \n",
        "Change the filepath and run.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "PS. It take much time on CIFAR dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm1ZTCbAdoXY",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dna9YuyVcb4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import collections\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def load_data(filepath):\n",
        "    '''\n",
        "    :param filepath: the path with filename\n",
        "    :return: Dataframe, Train data and test data\n",
        "    '''\n",
        "    data = np.load(filepath)\n",
        "    Xts = data['Xts']\n",
        "    Yts = data['Yts']\n",
        "    Xtr = data['Xtr']\n",
        "    Ytr = data['Str']\n",
        "\n",
        "    train_data = []\n",
        "    for i in range(Xtr.shape[0]):\n",
        "        train_data.append(Xtr[i].flatten())\n",
        "\n",
        "    test_data = []\n",
        "    for i in range(Xts.shape[0]):\n",
        "        test_data.append(Xts[i].flatten())\n",
        "\n",
        "    train_x = np.array(train_data)\n",
        "    test_x = np.array(test_data)\n",
        "    Yts = Yts.reshape(-1, 1)\n",
        "    Ytr = Ytr.reshape(-1, 1)\n",
        "    train_data = np.hstack((train_x, Ytr))\n",
        "    test_data = np.hstack((test_x, Yts))\n",
        "\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "    test_df = pd.DataFrame(test_data)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "class Tree(object):\n",
        "    '''\n",
        "    Define a decision tree\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.splitFeas = None\n",
        "        self.splitVal = None\n",
        "        self.leaf_value = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "    # Predict the category of sample by recursion\n",
        "    def predicValue(self, dataset):\n",
        "        if self.leaf_value is not None:\n",
        "            return self.leaf_value\n",
        "        elif dataset[self.splitFeas] <= self.splitVal:\n",
        "            return self.left.predicValue(dataset)\n",
        "        else:\n",
        "            return self.right.predicValue(dataset)\n",
        "\n",
        "class RandomForestClassifier(object):\n",
        "    def __init__(self, trees=10, max_depth=5, min_samples_split=2, min_samples_leaf=1,\n",
        "                 min_split_gain=0.0, subsample=1.0, random_state=None):\n",
        "        self.n_tree = trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.min_split_gain = min_split_gain\n",
        "        self.colsample_bytree = None\n",
        "        self.subsample = subsample\n",
        "        self.random_state = random_state\n",
        "        self.trees = dict()\n",
        "\n",
        "    def fit(self, dataset, targets):\n",
        "        targets = targets.to_frame(name=targets.name)\n",
        "\n",
        "\n",
        "        if self.random_state:\n",
        "            random.seed(self.random_state)\n",
        "\n",
        "        #Create the random seed\n",
        "        random_state_stages = random.sample(range(self.n_tree), self.n_tree)\n",
        "\n",
        "        # We randomly chose log2(features) of features\n",
        "        self.colsample_bytree = int(math.log(len(dataset.columns)))\n",
        "\n",
        "        for stage in range(self.n_tree):\n",
        "            print(\"iter: \"+str(stage+1))\n",
        "            random.seed(random_state_stages[stage])\n",
        "\n",
        "            # Randomly chose specific percentage of samples\n",
        "            subset_index = random.sample(range(len(dataset)), int(self.subsample * len(dataset)))\n",
        "\n",
        "            # Randomly chose the features\n",
        "            subcol_index = random.sample(dataset.columns.tolist(), self.colsample_bytree)\n",
        "\n",
        "            tempDataset = dataset.loc[subset_index, subcol_index].reset_index(drop=True)\n",
        "            tempTaget = targets.loc[subset_index, :].reset_index(drop=True)\n",
        "\n",
        "            tree = self.tree_fit(tempDataset, tempTaget, depth=0)\n",
        "            self.trees[stage] = tree\n",
        "\n",
        "    # Create decision tree by recursion\n",
        "    def tree_fit(self, dataset, targets, depth):\n",
        "        # Stop condition1: when leaf only have one category or the number smaller that the threshold\n",
        "        if len(targets[targets.columns[-1]].unique()) <= 1 \\\n",
        "                or len(dataset) <= self.min_samples_split:\n",
        "            tree = Tree()\n",
        "            tree.leaf_value = self.calLeafValue(targets[targets.columns[-1]])\n",
        "            return tree\n",
        "\n",
        "        # Stop condition2: when the depth reach the threshold\n",
        "        if depth < self.max_depth:\n",
        "            splitFea, splitValue, splitGain = self.chooseFeas(dataset, targets)\n",
        "            l_dataset, r_dataset, l_targets, r_targets = \\\n",
        "                self.split_dataset(dataset, targets, splitFea, splitValue)\n",
        "\n",
        "            tree = Tree()\n",
        "            # Stop condition3: sample's number is smaller than the threshold\n",
        "            if len(l_dataset) <= self.min_samples_leaf or \\\n",
        "                    len(r_dataset) <= self.min_samples_leaf or \\\n",
        "                    splitGain <= self.min_split_gain:\n",
        "                tree.leaf_value = self.calLeafValue(targets[targets.columns[-1]])\n",
        "                return tree\n",
        "            else:\n",
        "                tree.splitFeas = splitFea\n",
        "                tree.splitVal = splitValue\n",
        "                tree.left = self.tree_fit(l_dataset, l_targets, depth + 1)\n",
        "                tree.right = self.tree_fit(r_dataset, r_targets, depth + 1)\n",
        "                return tree\n",
        "        else:\n",
        "            tree = Tree()\n",
        "            tree.leaf_value = self.calLeafValue(targets[targets.columns[-1]])\n",
        "            return tree\n",
        "\n",
        "    # Choose the best way to split the data accoding the features, threshold and gains\n",
        "    def chooseFeas(self, dataset, targets):\n",
        "        splitGain = 1\n",
        "        splitFeature = None\n",
        "        splitVal = None\n",
        "\n",
        "        for fea in dataset.columns:\n",
        "            values = sorted(dataset[fea].unique().tolist())\n",
        "\n",
        "            # Calulate the gain of split and choose the best one\n",
        "            for value in values:\n",
        "                l_targets = targets[dataset[fea] <= value]\n",
        "                r_targets = targets[dataset[fea] > value]\n",
        "                splitGini = self.giniIndex(l_targets[l_targets.columns[-1]], r_targets[l_targets.columns[-1]])\n",
        "\n",
        "                if splitGini < splitGain:\n",
        "                    splitFeature = fea\n",
        "                    splitVal = value\n",
        "                    splitGain = splitGini\n",
        "        return splitFeature, splitVal, splitGain\n",
        "\n",
        "    # Set leaf value with the most frequent feas\n",
        "    @staticmethod\n",
        "    def calLeafValue(targets):\n",
        "        counts = collections.Counter(targets)\n",
        "        label = max(zip(counts.values(), counts.keys()))\n",
        "        return label[1]\n",
        "\n",
        "    # Use gini index to evaluate the split\n",
        "    @staticmethod\n",
        "    def giniIndex(l_targets, r_targets):\n",
        "        gain = 0\n",
        "        length = len(l_targets) + len(r_targets)\n",
        "        for targets in [l_targets, r_targets]:\n",
        "            gini = 1\n",
        "            len_targets = len(targets)\n",
        "            count = collections.Counter(targets)\n",
        "            for index in count:\n",
        "                prob = count[index] * 1.0 / len_targets\n",
        "                gini -= prob ** 2\n",
        "            gain += len(targets) * 1.0 / (length) * gini\n",
        "        return gain\n",
        "\n",
        "    # Split the dataset by threshold\n",
        "    @staticmethod\n",
        "    def split_dataset(dataset, targets, s_feas, s_value):\n",
        "        l_dataset = dataset[dataset[s_feas] <= s_value]\n",
        "        l_targets = targets[dataset[s_feas] <= s_value]\n",
        "        r_dataset = dataset[dataset[s_feas] > s_value]\n",
        "        r_targets = targets[dataset[s_feas] > s_value]\n",
        "        return l_dataset, r_dataset, l_targets, r_targets\n",
        "\n",
        "    # Predict the label\n",
        "    def predict(self, dataset):\n",
        "        Result = []\n",
        "        for index, row in dataset.iterrows():\n",
        "            predList = []\n",
        "            # Summary the prediction and choose the most frequency one\n",
        "            for stage, tree in self.trees.items():\n",
        "                predList.append(tree.predicValue(row))\n",
        "\n",
        "            labelCounts = collections.Counter(predList)\n",
        "            predLabel = max(zip(labelCounts.values(), labelCounts.keys()))[1]\n",
        "            Result.append(predLabel)\n",
        "        return np.array(Result)\n",
        "\n",
        "\n",
        "# Standerized the train and test data\n",
        "def standard(x_train, x_test):\n",
        "\n",
        "  std = np.std(x_train, keepdims=True)\n",
        "  mean = np.mean(x_train, keepdims=True)\n",
        "  x_train = (x_train-mean)/std\n",
        "  x_test = (x_test-mean)/std\n",
        "  return x_train, x_test\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #Change the path of file\n",
        "  df, test_df = load_data('/content/drive/My Drive/CIFAR.npz')\n",
        "  train_count = int(0.8 * len(df))\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "  test_acc = []\n",
        "  for i in range(1):\n",
        "    # shuffle the dataset and select 80% data\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    clf = RandomForestClassifier(trees=1,\n",
        "                    max_depth=5,\n",
        "                    min_samples_split=3,\n",
        "                    min_samples_leaf=3,\n",
        "                    subsample=0.2,\n",
        "                    random_state=233)\n",
        "\n",
        "    train_x = df.iloc[:, :-1].values\n",
        "    train_y = df.iloc[:, -1].values\n",
        "    test_x = test_df.iloc[:, :-1].values\n",
        "    test_y = test_df.iloc[:, -1].values\n",
        "    train_x, test_x = standard(train_x, test_x)\n",
        "    train_x = pd.DataFrame(train_x)\n",
        "    test_x = pd.DataFrame(test_x)\n",
        "\n",
        "    clf.fit(df.iloc[:train_count, :-1], df.iloc[:train_count, -1])\n",
        "        \n",
        "    train_acc.append(metrics.accuracy_score(df.iloc[:train_count, -1], clf.predict(df.iloc[:train_count, :-1])))\n",
        "    val_acc.append(metrics.accuracy_score(df.iloc[train_count:, -1], clf.predict(df.iloc[train_count:, :-1])))\n",
        "    test_acc.append(metrics.accuracy_score(test_df.iloc[:, -1], clf.predict(test_df.iloc[:, :-1])))\n",
        "\n",
        "\n",
        "  print(train_acc,np.mean(train_acc))\n",
        "  print(val_acc, np.mean(val_acc))\n",
        "  print(test_acc, np.mean(test_acc))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAit6odAf7Em",
        "colab_type": "text"
      },
      "source": [
        "## SVM baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT81lsPff_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# load the data, if dataset is CIFAR combine three channel\n",
        "def load_data(filepath,dataset = 'MINIST'):\n",
        "    data = np.load(filepath)\n",
        "\n",
        "    Xts = data['Xts']\n",
        "    Yts = data['Yts']\n",
        "    Xtr = data['Xtr']\n",
        "    Ytr = data['Str']\n",
        "\n",
        "    train_data = []\n",
        "    if dataset == 'CIFAR':\n",
        "        for i in range(Xtr.shape[0]):\n",
        "            temp = Xtr[i,:,:,0]+Xtr[i,:,:,1]+Xtr[i,:,:,2]\n",
        "            train_data.append(temp.flatten())\n",
        "    else:\n",
        "        for i in range(Xtr.shape[0]):\n",
        "            train_data.append(Xtr[i].flatten())\n",
        "\n",
        "    test_data = []\n",
        "    if dataset =='CIFAR':\n",
        "        for i in range(Xts.shape[0]):\n",
        "            temp = Xts[i, :, :, 0] + Xts[i, :, :, 1] + Xts[i, :, :, 2]\n",
        "            test_data.append(temp.flatten())\n",
        "    else:\n",
        "        for i in range(Xts.shape[0]):\n",
        "            test_data.append(Xts[i].flatten())\n",
        "\n",
        "    train_x = np.array(train_data)\n",
        "    test_x = np.array(test_data)\n",
        "    print(train_x.shape, test_x.shape)\n",
        "\n",
        "    Yts = Yts.reshape(-1, 1)\n",
        "    Ytr = Ytr.reshape(-1, 1)\n",
        "    train_data = np.hstack((train_x, Ytr))\n",
        "    test_data = np.hstack((test_x, Yts))\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "    test_df = pd.DataFrame(test_data)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "# standerize the train and test dataset\n",
        "def standard(x_train, x_test):\n",
        "    std = np.std(x_train, keepdims=True)\n",
        "    mean = np.mean(x_train, keepdims=True)\n",
        "    x_train = (x_train-mean)/std\n",
        "    x_test = (x_test-mean)/std\n",
        "    return x_train, x_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df, test_df = load_data('/content/drive/My Drive/CIFAR.npz')\n",
        "    \n",
        "    # shuffle the dataset and select 80% of data\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    train_count = int(0.8 * len(df))\n",
        "    \n",
        "    clf = svm.SVC(C = 20, kernel='poly')\n",
        "\n",
        "    train_x = df.iloc[:train_count, :-1].values.tolist()\n",
        "    train_y = df.iloc[:train_count, -1].values.tolist()\n",
        "    val_x = df.iloc[0:train_count, :-1].values.tolist()\n",
        "    val_y = df.iloc[0:train_count, -1].values.tolist()\n",
        "    test_x = test_df.iloc[:, :-1].values.tolist()\n",
        "    test_y = test_df.iloc[:, -1].values.tolist()\n",
        "\n",
        "    train_x, test_x = standard(train_x,test_x)\n",
        "\n",
        "    clf.fit(train_x, train_y)\n",
        "    print(len(train_y))\n",
        "    print(clf.score(train_x, train_y),clf.score(val_x,val_y),clf.score(test_x,test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}