{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CNN_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naah1wWjYVtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "import time\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#File path\n",
        "#Please set the path to your own path\n",
        "current = '/content/drive/My Drive/'\n",
        "\n",
        "# Create CNN model for FashionMNIST datasets\n",
        "def create_model():\n",
        "    \n",
        "    model = Sequential([\n",
        "    Conv2D(32,(5,5),padding='same',input_shape=(32,32,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Conv2D(32,(5,5),activation='relu'),\n",
        "    AveragePooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    Conv2D(64,(5,5),padding='same',activation='relu'),\n",
        "    Conv2D(64,(5,5),activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(512,activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3,activation='softmax')])\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "#Set label to one-hot catogory\n",
        "def one_hot_embedding(Str, Yts):\n",
        "    Str = to_categorical(Str, num_classes=3)\n",
        "    Yts = to_categorical(Yts, num_classes=3)\n",
        "    return Str, Yts\n",
        "\n",
        "\n",
        " # Calculate transition matrix\n",
        "def calculate_T(y_pred):\n",
        "    \n",
        "    T = np.zeros((3, 3))\n",
        "    index = int(y_pred.shape[0] *0.9)\n",
        "    \n",
        "    x_max_0 = y_pred[y_pred[:, 0].argsort()][index]\n",
        "    x_max_1 = y_pred[y_pred[:, 1].argsort()][index]\n",
        "    x_max_2 = y_pred[y_pred[:, 2].argsort()][index]\n",
        "\n",
        "    T[0][0] = 1.0 - x_max_0[1]-x_max_0[2]\n",
        "    T[0][1] = x_max_0[1]\n",
        "    T[0][2] = x_max_0[2]\n",
        "\n",
        "    T[1][0] = x_max_1[0]\n",
        "    T[1][1] = 1.0 - x_max_1[0] - x_max_1[2]\n",
        "    T[1][2] = x_max_1[2]\n",
        "\n",
        "    T[2][0] = x_max_2[0]\n",
        "    T[2][1] = x_max_2[1]\n",
        "    T[2][2] = 1.0 - x_max_2[0]-x_max_2[1]\n",
        "    return T\n",
        "\n",
        "# Add the transition matrix to our model\n",
        "def Addtran(T):\n",
        "\n",
        "        T = K.constant(T)\n",
        "\n",
        "        def loss(y_true, y_pred):\n",
        "            y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "            y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "            return -K.sum(y_true * K.log(K.dot(y_pred, T)), axis=-1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#Standardization, in order to increase the accuracy\n",
        "def standard(Xtr, Xts):\n",
        "    \n",
        "\n",
        "    std = np.std(Xtr, keepdims=True)\n",
        "    mean = np.mean(Xtr, keepdims=True)\n",
        "    Xtr = (Xtr - mean) / std\n",
        "    Xts = (Xts - mean) / std\n",
        "    return Xtr, Xts\n",
        "\n",
        "\n",
        "# load the dataset and calculate the transition matrix\n",
        "def Compute_T():\n",
        "    # load data\n",
        "    fpath1 = os.path.join(current,'CIFAR.npz')\n",
        "    dataset = np.load(fpath1)\n",
        "    \n",
        "    Xtr_val = dataset['Xtr']\n",
        "    Str_val = dataset['Str']\n",
        "    Xts = dataset['Xts']\n",
        "    Yts = dataset['Yts']\n",
        "\n",
        "    #Standardization\n",
        "    Xtr_val, Xts = standard(Xtr_val, Xts)\n",
        "\n",
        "    Str_val, Yts = one_hot_embedding(Str_val, Yts)\n",
        "    \n",
        "    # Random sample\n",
        "    a = np.argsort(np.random.random(Str_val.shape[0]))\n",
        "    Xtr_val = Xtr_val[a]\n",
        "    Str_val = Str_val[a]\n",
        "\n",
        "    Total = len(Xtr_val)\n",
        "    S = int(Total*0.8)\n",
        "\n",
        "    # Choose 80% of data for trainning, the other 20% for valuating.\n",
        "    Xtr_train = Xtr_val[:S]\n",
        "    Str_train = Str_val[:S]\n",
        "    Xtr_valid = Xtr_val[S:]\n",
        "    Str_valid = Str_val[S:]\n",
        "\n",
        "    Xtr_train = Xtr_train.reshape(Xtr_train.shape[0], 32, 32,3)\n",
        "    Xtr_valid = Xtr_valid.reshape(Xtr_valid.shape[0], 32, 32,3)\n",
        "\n",
        "    Xts = Xts.reshape(Xts.shape[0], 32, 32,3)\n",
        "\n",
        "    model = create_model()\n",
        "    \n",
        "    fpath = os.path.join(current,'CNNmodel.hdf5')\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer = keras.optimizers.rmsprop(lr=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Train the model and compare model in every epoch, save the best model to file\n",
        "    checkpointer = ModelCheckpoint(filepath=fpath, mode = 'max', monitor='val_acc',verbose = 1, save_best_only=True)\n",
        "\n",
        "    model.fit(Xtr_train,\n",
        "              Str_train,\n",
        "              epochs=50,\n",
        "              batch_size=1024,\n",
        "              validation_data=(Xts,Yts),\n",
        "              callbacks=[checkpointer],\n",
        "              )\n",
        "    model.load_weights(fpath)\n",
        "    time_1 = time.time()\n",
        "    print('The validation accuracy is :')\n",
        "    \n",
        "    # Get accuracy\n",
        "    accuracy = model.evaluate(Xtr_valid,Str_valid)[1]\n",
        "    print(accuracy)\n",
        "    print('The test accuracy is :')\n",
        "    accuracy1 = model.evaluate(Xts,Yts)[1]\n",
        "    print(accuracy1)\n",
        "    time_2 = time.time()\n",
        "    \n",
        "    # Use the predict result to compute transition\n",
        "    result = model.predict(Xtr_train)\n",
        "    T = calculate_T(result)\n",
        "    print(T)\n",
        "    return T,accuracy,accuracy1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZPeYvFGYVti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "af9aee28-94ff-44c6-b816-401e38aba985"
      },
      "source": [
        "# If you want to calculate your own transition matrix, set 'Yes'\n",
        "# If you want to test transition matrix provided by lecturer, set 'No' and skip\n",
        "COMPUTE = 'Yes'\n",
        "\n",
        "if(COMPUTE == 'Yes'):\n",
        "\n",
        "    Trans = np.zeros((10,3,3))\n",
        "\n",
        "    valid_accuracy = np.zeros((10))\n",
        "    test_accuracy = np.zeros((10))\n",
        "    \n",
        "    #Run for 10 times, and get mean value\n",
        "    for i in range(10):\n",
        "        \n",
        "        T,accu,accu1 = Compute_T()\n",
        "        Trans[i] = T\n",
        "        valid_accuracy[i] = accu\n",
        "        test_accuracy[i] = accu1\n",
        "        print('The valid accuracy is :')\n",
        "        print(valid_accuracy[i])\n",
        "        \n",
        "    #Get mean validation accuracy\n",
        "    print('The valid accuracy is :')\n",
        "    print(np.mean(valid_accuracy))\n",
        "    # Get the standard deviation of validation accuracy\n",
        "    print('The valid std is :')\n",
        "    print(valid_accuracy.std())\n",
        "    # Get mean test accuracy\n",
        "    print('The test accuracy is :')\n",
        "    print(np.mean(test_accuracy))\n",
        "    # Get the standard deviation of test accuracy\n",
        "    print('The test std is :')\n",
        "    print(test_accuracy.std())\n",
        "    # Get mean transition matrix\n",
        "    print(np.mean(Trans,axis=0))\n",
        "    # Get the standard deviation of test accuracy\n",
        "    print(np.std(Trans,axis=0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12000/12000 [==============================] - 1s 84us/step - loss: 1.0920 - acc: 0.3739 - val_loss: 1.0482 - val_acc: 0.5580\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.58200\n",
            "Epoch 11/50\n",
            "12000/12000 [==============================] - 1s 84us/step - loss: 1.0921 - acc: 0.3776 - val_loss: 1.0431 - val_acc: 0.5693\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.58200\n",
            "Epoch 12/50\n",
            "12000/12000 [==============================] - 1s 84us/step - loss: 1.0902 - acc: 0.3838 - val_loss: 1.0377 - val_acc: 0.6180\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.58200 to 0.61800, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 13/50\n",
            "12000/12000 [==============================] - 1s 84us/step - loss: 1.0895 - acc: 0.3840 - val_loss: 1.0365 - val_acc: 0.6147\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.61800\n",
            "Epoch 14/50\n",
            " 8192/12000 [===================>..........] - ETA: 0s - loss: 1.0873 - acc: 0.3893"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9259cf99654a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompute_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mTrans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ef7301e92057>\u001b[0m in \u001b[0;36mCompute_T\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m               )\n\u001b[1;32m    148\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIzbmvsfYVtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e46da45f-fbd8-4e53-aa72-60d724c375b8"
      },
      "source": [
        "valid_accuracy = np.zeros((10))\n",
        "test_accuracy = np.zeros((10))\n",
        "print(np.mean(Trans,axis=0))\n",
        "print(np.std(Trans,axis=0))\n",
        "T = np.mean(Trans,axis=0)\n",
        "print(T)\n",
        "\n",
        "# Run for 10 times, and get mean value\n",
        "for i in range(10):  \n",
        "\n",
        "    dataset = np.load(current+'CIFAR.npz')\n",
        "    Xtr_val = dataset['Xtr']\n",
        "    Str_val = dataset['Str']\n",
        "    Xts = dataset['Xts']\n",
        "    Yts = dataset['Yts']\n",
        "\n",
        "    Xtr_val, Xts = standard(Xtr_val, Xts)\n",
        "    Str_val, Yts = one_hot_embedding(Str_val, Yts)\n",
        "    a = np.argsort(np.random.random(Str_val.shape[0]))\n",
        "    Xtr_val = Xtr_val[a]\n",
        "    Str_val = Str_val[a]\n",
        "\n",
        "    Total = len(Xtr_val)\n",
        "    S = int(Total*0.8)\n",
        "\n",
        "    Xtr_train = Xtr_val[:S]\n",
        "    Str_train = Str_val[:S]\n",
        "    Xtr_valid = Xtr_val[S:]\n",
        "    Str_valid = Str_val[S:]\n",
        "    Xtr_train = Xtr_train.reshape(Xtr_train.shape[0], 32, 32, 3)\n",
        "    Xtr_valid = Xtr_valid.reshape(Xtr_valid.shape[0], 32, 32, 3)\n",
        "    Xts = Xts.reshape(Xts.shape[0], 32, 32, 3)\n",
        "    model = create_model()\n",
        "    fpath = os.path.join(current,'CNNmodel.hdf5')\n",
        "\n",
        "\n",
        "    # Train the model and compare model in every epoch, save the best model to file\n",
        "    T1 = np.array([[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]])\n",
        "    T2 = np.array([[0.4,0.3,0.3],[0.3,0.4,0.3],[0.3,0.3,0.4]])\n",
        "\n",
        "    # Add transition matrix to our model\n",
        "    # If you want to choose transition matrix provided above, change 'T' to 'T1' or 'T2'\n",
        "    model.compile(loss=Addtran(T),\n",
        "                  optimizer = keras.optimizers.rmsprop(lr=1e-4),\n",
        "                  metrics=['accuracy'])   \n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=fpath, mode = 'max', monitor='val_acc',verbose = 1, save_best_only=True)\n",
        "\n",
        "    model.fit(Xtr_train,\n",
        "              Str_train,\n",
        "              epochs=50,\n",
        "              batch_size=1024,\n",
        "              validation_data=(Xts,Yts),\n",
        "              callbacks=[checkpointer],\n",
        "              )\n",
        "    time_1 = time.time()\n",
        "\n",
        "    model.load_weights(\"CNNmodel.hdf5\")\n",
        "    print('The validation accuracy is :')\n",
        "    accuracy = model.evaluate(Xtr_valid,Str_valid)[1]\n",
        "    print(accuracy)\n",
        "    print('The test accuracy is :')\n",
        "    accuracy1 = model.evaluate(Xts,Yts)[1]\n",
        "    print(accuracy1)\n",
        "    time_2 = time.time()\n",
        "    valid_accuracy[i] = accuracy\n",
        "    test_accuracy[i] = accuracy1\n",
        "\n",
        "#Get mean validation accuracy\n",
        "print('The valid accuracy is :')\n",
        "print(np.mean(valid_accuracy))\n",
        "# Get the standard deviation of validation accuracy\n",
        "print('The valid std is :')\n",
        "print(valid_accuracy.std())\n",
        "\n",
        "#Get mean validation accuracy\n",
        "print('The test accuracy is :')\n",
        "print(np.mean(test_accuracy))\n",
        "# Get the standard deviation of test accuracy\n",
        "print('The test std is :')\n",
        "print(test_accuracy.std())\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.04019046 0.04061811 0.01919143]\n",
            " [0.02954744 0.03966265 0.03078991]\n",
            " [0.02309527 0.03614412 0.04076061]]\n",
            "[[0.12057139 0.12185432 0.05757429]\n",
            " [0.08864231 0.11898795 0.09236974]\n",
            " [0.0692858  0.10843237 0.12228184]]\n",
            "[[0.04019046 0.04061811 0.01919143]\n",
            " [0.02954744 0.03966265 0.03078991]\n",
            " [0.02309527 0.03614412 0.04076061]]\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        2432      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 12, 12, 32)        25632     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_5 (Average (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 6, 6, 64)          51264     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 2, 2, 64)          102464    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 314,915\n",
            "Trainable params: 314,915\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 12000 samples, validate on 3000 samples\n",
            "Epoch 1/50\n",
            "12000/12000 [==============================] - 2s 149us/step - loss: 3.4059 - acc: 0.3333 - val_loss: 3.4020 - val_acc: 0.3340\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.33400, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 2/50\n",
            "12000/12000 [==============================] - 1s 84us/step - loss: 3.4045 - acc: 0.3412 - val_loss: 3.3933 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.33400 to 0.43767, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 3/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4041 - acc: 0.3467 - val_loss: 3.3868 - val_acc: 0.4870\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.43767 to 0.48700, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 4/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4032 - acc: 0.3580 - val_loss: 3.3790 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.48700\n",
            "Epoch 5/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4024 - acc: 0.3527 - val_loss: 3.3744 - val_acc: 0.5307\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.48700 to 0.53067, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 6/50\n",
            "12000/12000 [==============================] - 1s 83us/step - loss: 3.4020 - acc: 0.3592 - val_loss: 3.3711 - val_acc: 0.5323\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.53067 to 0.53233, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 7/50\n",
            "12000/12000 [==============================] - 1s 83us/step - loss: 3.4024 - acc: 0.3609 - val_loss: 3.3678 - val_acc: 0.5290\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.53233\n",
            "Epoch 8/50\n",
            "12000/12000 [==============================] - 1s 81us/step - loss: 3.4018 - acc: 0.3609 - val_loss: 3.3632 - val_acc: 0.5377\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.53233 to 0.53767, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 9/50\n",
            "12000/12000 [==============================] - 1s 83us/step - loss: 3.4014 - acc: 0.3607 - val_loss: 3.3630 - val_acc: 0.5153\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.53767\n",
            "Epoch 10/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4009 - acc: 0.3638 - val_loss: 3.3597 - val_acc: 0.5320\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.53767\n",
            "Epoch 11/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4008 - acc: 0.3636 - val_loss: 3.3625 - val_acc: 0.5280\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.53767\n",
            "Epoch 12/50\n",
            "12000/12000 [==============================] - 1s 82us/step - loss: 3.4008 - acc: 0.3639 - val_loss: 3.3601 - val_acc: 0.5380\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.53767 to 0.53800, saving model to /content/drive/My Drive/CNNmodel.hdf5\n",
            "Epoch 13/50\n",
            " 4096/12000 [=========>....................] - ETA: 0s - loss: 3.3994 - acc: 0.3628"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-21f322ffc5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m               )\n\u001b[1;32m     56\u001b[0m     \u001b[0mtime_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VovdH9e9YVtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}